---
title: "PracticalMLCourseProject"
author: "Sauvik Dutta"
date: "12/6/2017"
output: html_document
---

#### Problem statement

> *Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
Goal is to predict Classes of the testing set!*

Load all necessary packages!

```{r, message=FALSE}
library(fbr); library(data.table); library(caret); library(dplyr); library(randomForest)
```

Load the dataset, get a glimpse of it and check that the structure of the object is something we work with?

```{r}
# Note I have to use the "fbr::with_proxy" to load this data. 
training <- fbr::with_proxy(fread('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'))
testing <- fbr::with_proxy(fread('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'))
head(training)
```

#### Initial observations:

* The first column is basically the row index so we can either convert that or conveniently choose to ignore that.
* Num rows in training -> 19622
* Num rows in testing -> 20
* Num columns in both -> 160
* Multiple **NA** and blank "" values (eg. "amplitude_pitch_belt" and "kurtosis_yaw_belt" respectively)
* Not all columns are numerical like "user_name" and "new_window" (could be categorical?)

Let's look at the structure of the *training* object!

```{r}
class(training)
```

The structure will be converted to data.frame just for convenience!

```{r, message=FALSE, results='hide'}
training_df <- as.data.frame(training)
testing_df <- as.data.frame(testing)
```

Let's now get a glimpse of the strcuture of each and every column. Since there are only 160 features/regressors/columns, let's just print all of them.

```{r}
str(training_df, list.len=ncol(training_df))
```

#### Observations:
* Many **character** type columns
* Mixture of **int** and **numeric** for quantitative columns
* **classe** is the class variable that we'll need to predict the data type of which is **character** (this needs to be converted to **Factor**)

Let's look at some columns which are of **character** type! 


* ***"new_window"***

```{r}
table(training_df$new_window, useNA="always")
```

Let's now look at what this column looks like in the testing set

```{r}
table(training_df$new_window, useNA="always")
```

There's a high chance that this feature is not useful at all and may just end up overfitting as ~98% of the data belongs to one category

* ***"kurtosis_roll_belt"***

```{r}
table(training_df$new_window, useNA="always")
```

19216 blank entries! We will have to do some processing here like say fill out the blank entries with column averages or medians.
Let's see how this looks for the testing set.

```{r}
table(testing_df$new_window, useNA="always")
```

Everything is blank, so even if we replace blank values with some column statistic, we'll probably have to fill the testing set with 0. We can also choose to ignore this column entirely, but if I am using a tree as a model, the algorithm should eliminate unnecessary features This is better than removing the column altogether!

* ***"max_roll_belt"***

```{r}
table(training_df$max_roll_belt, useNA="always")
```

Again, we have about 19216 NA's!
The trend seems to continue, so let's look at some other randomly chosen column!

* ***"amplitude_yaw_belt"***
```{r}
table(training_df$amplitude_yaw_belt, useNA="always")
```

This is interesting, because now this also shows **#DIV/0!** as popping up. 

### Anomialies found:
* DIV/0
* NA
* ""

We'll need to get rid of these! Let's convert every **character** to **numeric** (this will also convert "" to NA). Even before that 'et's get rid of the most obvious unneccessary columns:

* **v1**
* **user_name**
* **raw_timestamp_part_1** (since this exercise should not be time dependednt unless it's forecasting or something)
* **raw_timestamp_part_2**
* **cvtd_timestamp**
* **new_window**
* **num_window** (the number of the window should not matter)


```{r}
training_df_trimmed <- training_df[, !names(training_df) %in% c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")]
testing_df_trimmed <- testing_df[, !names(testing_df) %in% c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")]
```


Convert **character** to numeric and **classe** to **Factor**!

```{r, warning=FALSE, results='hide', message=FALSE}
training_df_trimmed_uniform <- data.frame(sapply(training_df_trimmed, function(x) as.numeric(as.character(x))))
testing_df_trimmed_uniform <- data.frame(sapply(testing_df_trimmed, function(x) as.numeric(as.character(x))))
training_df_trimmed_uniform$classe <- as.factor(training_df$classe)
```

Check the dataset:

```{r}
str(training_df_trimmed_uniform, list.len=ncol(training_df_trimmed_uniform))
```

Replace NULLs with column averages!

```{r}
training_df_trimmed_clean_v1 <- data.frame(lapply(training_df_trimmed_uniform[, -153], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))
training_df_trimmed_clean_v1$classe <- training_df_trimmed_uniform$classe
testing_df_trimmed_clean_v1 <- data.frame(lapply(testing_df_trimmed_uniform[, -153], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))
str(training_df_trimmed_clean_v1, list.len=ncol(training_df_trimmed_clean_v1))
```

Remove further useless columns that are still remaining (because if there are NULLs after replacing with column averages, then those are probably erroneous data points)!

```{r}
training_df_trimmed_clean_v2 <- training_df_trimmed_clean_v1[, !names(training_df_trimmed_clean_v1) %in% c("kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "kurtosis_yaw_forearm", "skewness_yaw_forearm")]
testing_df_trimmed_clean_v2 <- testing_df_trimmed_clean_v1[, !names(testing_df_trimmed_clean_v1) %in% c("kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "kurtosis_yaw_forearm", "skewness_yaw_forearm")]
```

Time to train the model! We'll use caret's rf implementation. We can use other tree based models as well but rf should be just good to get predictions on classes! 

Get Predictions!

```{r}
model <- randomForest(classe~., data=training_df_trimmed_clean_v2, importance=TRUE, ntree=50)
```

```{r}
x_test <- data.frame(lapply(testing_df_trimmed_clean_v2, function(x) ifelse(is.na(x), 0, x)))
predict(model, x_test)
```

Looks like a satisfying prediction. Exploring other options may be valuable but Trees should usually do a good job of multi-class prediction in most cases. 













